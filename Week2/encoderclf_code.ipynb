{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "90030a1c",
      "metadata": {},
      "source": [
        "### 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "J3qiuUJjgmGj",
      "metadata": {
        "id": "J3qiuUJjgmGj"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchdata torchtext==0.12 torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7038ce",
      "metadata": {},
      "source": [
        "## Transformer Encoder classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7d4db5bf",
      "metadata": {
        "id": "7d4db5bf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import pandas as pd\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "import warnings\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "\n",
        "# Set to False to skip notebook execution (e.g. for debugging)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "NfFNB7oqkOEN",
      "metadata": {
        "id": "NfFNB7oqkOEN"
      },
      "outputs": [],
      "source": [
        "device=torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e791d790",
      "metadata": {
        "id": "e791d790"
      },
      "source": [
        "# Part1: Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf81363f",
      "metadata": {
        "id": "bf81363f"
      },
      "source": [
        "## Model Architecture  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0367c79f",
      "metadata": {
        "id": "0367c79f"
      },
      "outputs": [],
      "source": [
        "# N개 layer \n",
        "def clones(module, N):\n",
        "    \n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5c885e94",
      "metadata": {
        "id": "5c885e94"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Encoder clf\n",
        "임베딩->인코더->(batch_size,seq_length,d_model)\n",
        "embedding: positional encoding + embedding\n",
        "generator->linear(d_model, n_class) -> log_softmax -> (batch, n_class)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Encoderclf(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, src_embed, generator):\n",
        "        super().__init__() \n",
        "        self.encoder=encoder # 인코더 전체\n",
        "        self.src_embed=src_embed #embedding class\n",
        "        self.generator=generator\n",
        "        \n",
        "    def forward(self, src):\n",
        "        x=self.encoder(self.src_embed(src))\n",
        "        return x\n",
        "    def encode(self, src):\n",
        "        return self.encoder(self.src_embed(src))\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "df81b5bf",
      "metadata": {
        "id": "df81b5bf"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "\n",
        "    def __init__(self, d_model, n_class ):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model,n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= x.mean(dim=1) # batch ,d_model\n",
        "        x=self.proj(x) # batch, n_class\n",
        "        return log_softmax(x,dim=-1) # batch, n_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bc12167f",
      "metadata": {
        "id": "bc12167f"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.layers=clones(layer,N)\n",
        "        self.norm=LayerNorm(layer.size) #layer.size=d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x=layer(x)\n",
        "        return self.norm(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6d6ab99b",
      "metadata": {
        "id": "6d6ab99b"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    \"feature 차원 정규화\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True) \n",
        "        std = x.std(-1, keepdim=True) \n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cc7215c1",
      "metadata": {
        "id": "cc7215c1"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    dropout -> residual connection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "86da6d45",
      "metadata": {
        "id": "86da6d45"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6c92d7c5",
      "metadata": {
        "id": "6c92d7c5"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    \n",
        "    p_attn = scores.softmax(dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "        \n",
        "    return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e7901bdc",
      "metadata": {
        "id": "e7901bdc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "            \n",
        "        nbatches = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        #파이토치의 view는 사이즈가 -1로 설정되면 다른 차원으로부터 해당 값을 유추\n",
        "        # transpose -> e두개의 차원 교환\n",
        "        query, key, value = [\n",
        "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "            for lin, x in zip(self.linears, (query, key, value))\n",
        "        ]\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(\n",
        "            query, key, value, dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = (\n",
        "            x.transpose(1, 2)\n",
        "            .contiguous()\n",
        "            .view(nbatches, -1, self.h * self.d_k)\n",
        "        )\n",
        "        del query\n",
        "        del key\n",
        "        del value\n",
        "        return self.linears[-1](x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cf547802",
      "metadata": {
        "id": "cf547802"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.-Nonlinearity\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.w_1(x).relu()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f851b029",
      "metadata": {
        "id": "f851b029"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dfacc553",
      "metadata": {
        "id": "dfacc553"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "460e5768",
      "metadata": {
        "id": "460e5768"
      },
      "outputs": [],
      "source": [
        "def make_model(\n",
        "    src_vocab, target, N=4, d_model=16, d_ff=32, h=4, dropout=0.1\n",
        "):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = Encoderclf(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        Generator(d_model, target),\n",
        "    )\n",
        "\n",
        "    # This was important from their code.\n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "  \n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44bee536",
      "metadata": {
        "id": "44bee536"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "920e2eda",
      "metadata": {
        "id": "920e2eda"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer=get_tokenizer('basic_english')\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "# tokening\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text) \n",
        "\n",
        "# vocabulary (train set 에 대해서만)\n",
        "vocab_src = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\",'<blank>'])\n",
        "vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
        "\n",
        "# pipeline\n",
        "text_pipeline = lambda x: vocab_src(tokenizer(x)) # vocabulary 내 word 위치\n",
        "label_pipeline = lambda x: 1. if (x=='pos') else 0 # pos==1, neg==0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "06f4599a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06f4599a",
        "outputId": "3d38e8b8-713f-41f9-aa4a-8e880047818d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100684"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab_src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "33350048",
      "metadata": {
        "id": "33350048"
      },
      "outputs": [],
      "source": [
        "def collate_batch(\n",
        "\n",
        "     batch,\n",
        "     text_pipeline,\n",
        "     label_pipeline,\n",
        "     max_padding=1000,\n",
        "     pad_id=2,\n",
        "):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(torch.tensor(label_pipeline(_label), dtype=torch.int))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int )\n",
        "         text_list.append(\n",
        "               pad(\n",
        "                processed_text,\n",
        "                (\n",
        "                    0,\n",
        "                    max_padding - len(processed_text),\n",
        "                ),\n",
        "                value=pad_id,\n",
        "               )\n",
        "          )\n",
        "    src = torch.stack(text_list)\n",
        "    tgt = torch.stack(label_list)\n",
        "    return (tgt, src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bb8c356b",
      "metadata": {
        "id": "bb8c356b"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "def create_dataloader(\n",
        "    \n",
        "    vocab_src,\n",
        "    text_pipeline,\n",
        "    label_pipeline,\n",
        "    batch_size=64,\n",
        "    max_padding=128,\n",
        "    is_distributed=True,\n",
        "):\n",
        "    def collate_fn(batch):\n",
        "        return collate_batch(\n",
        "            \n",
        "            batch,\n",
        "            text_pipeline,\n",
        "            label_pipeline,\n",
        "            max_padding=max_padding,\n",
        "            pad_id=vocab_src.get_stoi()['<blank>'],\n",
        "        )\n",
        "\n",
        "    train_iter, test_iter=datasets.IMDB(split=('train','test'))\n",
        "\n",
        "    train_iter_map = to_map_style_dataset(\n",
        "        train_iter\n",
        "    )  # DistributedSampler needs a dataset len()\n",
        "    num_train = int(len(train_iter_map) * 0.95)\n",
        "    split_train_, split_valid_ = \\\n",
        "        random_split(train_iter_map, [num_train, len(train_iter_map) - num_train])\n",
        "    \n",
        "    train_dataloader = DataLoader(\n",
        "        split_train_,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "\n",
        "    valid_dataloader = DataLoader(\n",
        "        split_valid_,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "    )\n",
        "    \n",
        "    return train_dataloader, valid_dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07ee879",
      "metadata": {
        "id": "b07ee879"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e56a94a3",
      "metadata": {
        "id": "e56a94a3"
      },
      "outputs": [],
      "source": [
        "model=make_model(len(vocab_src),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1c5b39da",
      "metadata": {
        "id": "1c5b39da"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model.generator(model(text))\n",
        "        label = label.type(torch.LongTensor)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            predicted_label = model.generator(model(text))\n",
        "            label = label.type(torch.LongTensor)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "CwGWnqv4Y8tZ",
      "metadata": {
        "id": "CwGWnqv4Y8tZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "lr = 1e-3# learning rate\n",
        "optimizer = torch.optim.Adam(\n",
        "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
        ")\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = 0.6\n",
        "test_iter = IMDB(split='test')\n",
        "\n",
        "train_dataloader, valid_dataloader =create_dataloader(vocab_src, text_pipeline, label_pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "odkGUJJA6AdD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odkGUJJA6AdD",
        "outputId": "1a20c469-677f-46e1-d54f-8190f59f92f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 235.06s | valid accuracy    0.781 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 235.93s | valid accuracy    0.850 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 235.94s | valid accuracy    0.848 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 232.80s | valid accuracy    0.853 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 232.90s | valid accuracy    0.850 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 234.27s | valid accuracy    0.848 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 238.82s | valid accuracy    0.850 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 234.03s | valid accuracy    0.850 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 234.96s | valid accuracy    0.850 \n",
            "-----------------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 231.05s | valid accuracy    0.850 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3L_2K88ewa6b",
      "metadata": {
        "id": "3L_2K88ewa6b"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "        return collate_batch(\n",
        "            \n",
        "            batch,\n",
        "            text_pipeline,\n",
        "            label_pipeline,\n",
        "            max_padding=128,\n",
        "            pad_id=vocab_src.get_stoi()['<blank>'],\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "SuBul7z4pjde",
      "metadata": {
        "id": "SuBul7z4pjde"
      },
      "outputs": [],
      "source": [
        "test_dataset=to_map_style_dataset(test_iter)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "ODfodm9SpxWS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODfodm9SpxWS",
        "outputId": "05b677c4-1370-471e-ce58-a9b65d1d3e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy    0.819\n"
          ]
        }
      ],
      "source": [
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JmZs_WUjwoy8",
      "metadata": {
        "id": "JmZs_WUjwoy8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "44bee536"
      ],
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf9a6fefc0100232e69dba872cc3fae8072b269aae356d568f140f30cb548e8a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
